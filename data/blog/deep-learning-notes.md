Whenever a new deep learning result has come out, I've been very successful at rationalizing why I should ignore it. The rationalizations are always the same, "There just blackbox model-less approaches that will ultimately fail when used in anger."

This mindset of mine comes from two places. When I first learned about neural networks in the 1990's, I learned why they failed. Classifiers would appear to work with training and validation sets, only to fail in the real world. I remember a physics teacher explaining to me that a fighter jet classier appeared to work, because all the test data had a pink pixels that where absent from the real world data. Anyway the lesson was clear: neural networks only appear to work and you have no idea how well the work in practice.

In the early 00s I was picking up exercise equipment from Craigslist seller. He happened to be a Stanford trained machine learning engineer and was moving back to India. I recognized some of his books and told me I could have his entire library of 30 or so information theory and machine learning books. That chance encounter put me a entirely new intellectual direction.

I started working through,
