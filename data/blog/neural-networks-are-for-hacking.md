In the idealized version of software development, programmers are applied mathematicians. The development process starts with a rigorous specification. Then the software is written and there is a proof that demonstrates the code implements the specification.

This idea of software developers as applied mathematicians, originates from the 60s, when software development practices were disseminated through academic papers and programmers were seen as PhD. math geniuses.

As the discipline of programming moved out of academy and was adopted by hobbyists, this view changed. Programmers were no longer mathematicians, they were builders.

Programming went from a science to a craft, millions learned to program and software transformed the world.

In this transition, something was lost. Programs went from algorithms that could be verified, to large systems, with no specification that could only be understood through their emergent dynamic behavior.

We traded limited functionality we could understand deeply, for wide-reaching, powerful tools so complex we can't ever hope to completely comprehend them fully.

## Neural Networks are a Craft

Neural network development seems like some advanced mathematics. For one, everyone seems to hold a PhD. that makes them. They use complex math like tensors and stochastic gradient descent. New developments are published in academic papers, etc, etc. It is all very reminiscent of 1960s software development. There is an academic pretense around neural network methods that makes them seem more complex than they really are.

Neural networks, which are differentialable pipelines of linear and simple monotonic non-linear functions, are exploding in utility, in part because programmers can try out different pipelines without having to apply difficult mathematical reasoning.

Neural networks libraries, like `pytorch` make it easy to compose pipelines and get an intuitive sense for what problems they can solve, even without completely understanding the theory. Neural networks are for hacking, they're not for precise mathematical reasoning, as their academic treatments might lead one to believe.

What we are seeing now, is similar to what happened in the 80s and 90s. Neural networks are escaping the Ivory Tower. You can become a good deep learning practitioner by trying things. If it appears to work, you can see if it works in a wider class of tests. Maybe it works well enough to make money.

Neural networks compete in the same space as Bayesian methods, which are more limited because of how expensive the computation is. Again, we lose a deep understanding of what are programs will do, but we can solve nearly unimaginable number of problems well enough.

## FAFO

Neural networks are less a precise mathematical solution to a problem, and a more fuzzy way to program. They're useful when errors are tolerable, and it is not clear how to write a direct algorithm. Just like with regular programming, there often isn't a right answer. There is merely a solution that might work for your purposes and to know for sure you have to try it out.
